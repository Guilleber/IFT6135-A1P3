import os
import io
import torch
import torch.nn as nn
import numpy as np
from collections import Counter
-m pip install pytorch-nlp

from torchnlp.datasets import penn_treebank_dataset
data = penn_treebank_dataset(train=True)
chars = list(set(data))
data_size, vocab_size = len(data), len(chars)
print ('data has %d characters, %d unique.' % (data_size, vocab_size))
char_to_ix = { ch:i for i,ch in enumerate(chars) }
ix_to_char = { i:ch for i,ch in enumerate(chars) }


class RNN(nn.Module):
    def __init__(self, hid_size, seq_len, lr, vocab_size, num_layers):
        super(RNN, self).__init__() 
        self.hidden_size =  hid_size
        self.seq_length = seq_len 
        self.learning_rate = lr
        self.vocab_size = vocab_size
        self.num_layers = num_layers
        self.x, self.h, self.y, self.p = {}, {}, {}, {}
        self.Wx, self.Wh, self.Wy, self.bh, self.by = {}, {}, {}, {}, {}
        self.dWx, self.dWh, self.dWy, self.dbh, self.dby = {}, {}, {}, {}, {}

    def forward(self, inputs):
        self.x, self.h, self.y, self.p = {}, {}, {}, {}
        for t in range(len(inputs)):
            self.x[t] = np.zeros((self.vocab_size,1)) # one hot representation
            self.x[t][inputs[t]] = 1
            self.h[t][0,:] = np.tanh(np.dot(self.Wx, self.x[t]) + np.dot(self.Wh[0,:,:], self.h[t-1][0,:]) + self.bh[0,:]) 
            for j in range(1, self.num_layers):
                self.h[t][j,:] = np.tanh(np.dot(self.Wh[j,:,:], self.h[t-1][j,:]) + self.bh[j,:] + np.dot(self.Wh[j-1,:,:], self.h[t][j-1,:]))
            self.y[t] = np.dot(self.Wy, self.h[t][self.num_layers,:]) + self.by 
            self.p[t] = np.exp(self.y[t]) / np.sum(np.exp(self.y[t]))
            self.loss += -np.log(self.p[t][targets[t],0]) # cross-entropy loss
    
    def init_weight(self):
        self.Wx = np.random.rand(self.hidden_size, self.vocab_size)*np.sqrt(1/(self.hidden_size + self.vocab_size))
        self.Wh = np.random.rand(self.num_layers, self.hidden_size, self.hidden_size)*np.sqrt(1/(self.hidden_size + self.hidden_size))
        self.bh = np.random.rand(self.hidden_size)*np.sqrt(1/self.hidden_size)
        self.by = np.random.rand(self.vocab_size)*np.sqrt(1/self.vocab_size)
 

def generate(rnn, seed_ix, n):
    xg = np.zeros((vocab_size, 1))
    xg[seed_ix] = 1
    ixes = []
    hg = np.zeros((rnn.num_layers - 1, rnn.vocab_size))
    for t in range(n):
        hg[0,:] = np.tanh(np.dot(rnn.Wx, xg) + np.dot(rnn.Wh[0,:,:], hg[0,:]) + rnn.bh[0,:]) 
        for j in range(1, rnn.num_layers):
            hg[j,:] = np.tanh(np.dot(rnn.Wh[j,:,:], hg[j,:]) + rnn.bh[j,:] + np.dot(rnn.Wh[j-1,:,:], rnn.hg[j-1,:]))
        yg = np.dot(rnn.Wy, hg[rnn.num_layers,:]) + rnn.by
        pg = np.exp(yg) / np.sum(np.exp(yg))
        ix = np.random.choice(range(vocab_size), p=p.ravel())
        xg = np.zeros((vocab_size, 1))
        xg[ix] = 1
        ixes.append(ix)
    return ixes


def train(rnn, inputs, targets, hprev):
    x, h, y, p = {}, {}, {}, {}
    h[-1] = np.copy(hprev)
    loss = 0
    x, h, p, y, loss = rnn.forward(inputs)
    #Backprop through time
    dWxh, dWhh, dWhy = np.zeros_like(rnn.Wxh), np.zeros_like(rnn.Whh), np.zeros_like(rnn.Why)
    dbh, dby = np.zeros_like(rnn.bh), np.zeros_like(rnn.by)
    dhnext = np.zeros_like(h[0])
    for t in reversed(range(len(inputs))):
        dy = np.copy(p[t])
        dy[targets[t]] -= 1 
        dWhy += np.dot(dy, h[t].T)
        dby += dy
        dh = np.dot(rnn.Why.T, dy) + dhnext 
        dhraw = (1 - h[t] * h[t]) * dh 
        dbh += dhraw
        dWxh += np.dot(dhraw, x[t].T)
        dWhh += np.dot(dhraw, h[t-1].T)
        dhnext = np.dot(rnn.Whh.T, dhraw)
    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:
        np.clip(dparam, -5, 5, out=dparam) 
    return loss, dWxh, dWhh, dWhy, dbh, dby, h[len(inputs)-1]

def test(rnn, data):
    n, p = 0, 0
    smooth_loss = -np.log(1.0/vocab_size)*rnn.seq_length 
    while n<1000:
        if p+rnn.seq_length+1 >= len(data) or n == 0: 
            hprev = np.zeros((rnn.hidden_size,1)) 
            p = 0
        inputs = [char_to_ix[ch] for ch in data[p:p+rnn.seq_length]]
        targets = [char_to_ix[ch] for ch in data[p+1:p+rnn.seq_length+1]]
        #Genrate samples to have an idea of performance
        #if n % 100 == 0:
        #    sample_ix = generate(rnn, hprev, inputs[0], 200)
        #    txt = ''.join(ix_to_char[ix] for ix in sample_ix)
        #    print ('----\n %s \n----' % (txt, ))
        loss, dWxh, dWhh, dWhy, dbh, dby, hprev = train(rnn, inputs, targets, hprev)
        smooth_loss = smooth_loss * 0.999 + loss * 0.001
        if n % 100 == 0: print ('iter %d, loss: %f' % (n, smooth_loss))
        for param, dparam in zip([rnn.Wxh, rnn.Whh, rnn.Why, rnn.bh, rnn.by], [dWxh, dWhh, dWhy, dbh, dby]):
            mem += dparam * dparam
            param += -rnn.learning_rate * dparam 
        p += rnn.seq_length 
        n += 1
