import os
import io
import torch
import torch.nn as nn
import numpy as np
from collections import Counter
-m pip install pytorch-nlp

from torchnlp.datasets import penn_treebank_dataset
data = penn_treebank_dataset(train=True)
chars = list(set(data))
data_size, vocab_size = len(data), len(chars)
print ('data has %d characters, %d unique.' % (data_size, vocab_size))
char_to_ix = { ch:i for i,ch in enumerate(chars) }
ix_to_char = { i:ch for i,ch in enumerate(chars) }


class RNN(nn.Module):
    def __init__(self):
        super(RNN, self).__init__() 
        self.hidden_size = 100 
        self.seq_length = 25 
        self.learning_rate = 1e-1

        self.Wxh = np.random.randn(self.hidden_size, vocab_size)*0.01
        self.Whh = np.random.randn(self.hidden_size, self.hidden_size)*0.01  
        self.Why = np.random.randn(vocab_size, self.hidden_size)*0.01  
        self.bh = np.zeros((self.hidden_size, 1)) 
        self.by = np.zeros((vocab_size, 1))  
        self.mWxh, self.mWhh, self.mWhy = np.zeros_like(self.Wxh), np.zeros_like(self.Whh), np.zeros_like(self.Why)
        self.mbh, self.mby = np.zeros_like(self.bh), np.zeros_like(self.by) 

    def forward(self, inputs):
        x, h, y, p = {}, {}, {}, {}
        for t in range(len(inputs)):
            x[t] = np.zeros((vocab_size,1)) # one hot representation
            x[t][inputs[t]] = 1
            h[t] = np.tanh(np.dot(self.Wxh, x[t]) + np.dot(self.Whh, h[t-1]) + self.bh) 
            y[t] = np.dot(self.Why, h[t]) + self.by 
            p[t] = np.exp(y[t]) / np.sum(np.exp(y[t]))
            loss += -np.log(p[t][targets[t],0]) # cross-entropy loss
        return x, h, p, y, loss
 

def generate(rnn, h, seed_ix, n):
    x = np.zeros((vocab_size, 1))
    x[seed_ix] = 1
    ixes = []
    for t in range(n):
        h = np.tanh(np.dot(rnn.Wxh, x) + np.dot(rnn.Whh, h) + rnn.bh)
        y = np.dot(rnn.Why, h) + rnn.by
        p = np.exp(y) / np.sum(np.exp(y))
        ix = np.random.choice(range(vocab_size), p=p.ravel())
        x = np.zeros((vocab_size, 1))
        x[ix] = 1
        ixes.append(ix)
    return ixes


def train(rnn, inputs, targets, hprev):
    x, h, y, p = {}, {}, {}, {}
    h[-1] = np.copy(hprev)
    loss = 0
    x, h, p, y, loss = rnn.forward(inputs)
    #Backprop through time
    dWxh, dWhh, dWhy = np.zeros_like(rnn.Wxh), np.zeros_like(rnn.Whh), np.zeros_like(rnn.Why)
    dbh, dby = np.zeros_like(rnn.bh), np.zeros_like(rnn.by)
    dhnext = np.zeros_like(h[0])
    for t in reversed(range(len(inputs))):
        dy = np.copy(p[t])
        dy[targets[t]] -= 1 
        dWhy += np.dot(dy, h[t].T)
        dby += dy
        dh = np.dot(rnn.Why.T, dy) + dhnext 
        dhraw = (1 - h[t] * h[t]) * dh 
        dbh += dhraw
        dWxh += np.dot(dhraw, x[t].T)
        dWhh += np.dot(dhraw, h[t-1].T)
        dhnext = np.dot(rnn.Whh.T, dhraw)
    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:
        np.clip(dparam, -5, 5, out=dparam) 
    return loss, dWxh, dWhh, dWhy, dbh, dby, h[len(inputs)-1]

def test(rnn, data):
    n, p = 0, 0
    smooth_loss = -np.log(1.0/vocab_size)*rnn.seq_length 
    while n<1000:
        if p+rnn.seq_length+1 >= len(data) or n == 0: 
            hprev = np.zeros((rnn.hidden_size,1)) 
            p = 0
        inputs = [char_to_ix[ch] for ch in data[p:p+rnn.seq_length]]
        targets = [char_to_ix[ch] for ch in data[p+1:p+rnn.seq_length+1]]
        #Genrate samples to have an idea of performance
        #if n % 100 == 0:
        #    sample_ix = generate(rnn, hprev, inputs[0], 200)
        #    txt = ''.join(ix_to_char[ix] for ix in sample_ix)
        #    print ('----\n %s \n----' % (txt, ))
        loss, dWxh, dWhh, dWhy, dbh, dby, hprev = train(rnn, inputs, targets, hprev)
        smooth_loss = smooth_loss * 0.999 + loss * 0.001
        if n % 100 == 0: print ('iter %d, loss: %f' % (n, smooth_loss))
        for param, dparam in zip([rnn.Wxh, rnn.Whh, rnn.Why, rnn.bh, rnn.by], [dWxh, dWhh, dWhy, dbh, dby]):
            mem += dparam * dparam
            param += -rnn.learning_rate * dparam 
        p += rnn.seq_length 
        n += 1
